batch inserts
partitioning
compression

Proterly scheme all of the tables
primary keys
materialized views

indexing and projections
performance monitoring : system metrics, query performace and resource limiting

security configuration
vacuuming and compaction
indexing
monitoring alertx

Add it to prometeus configuration

scrape_configs:
  - job_name: 'clickhouse'
    static_configs:
      - targets: ['localhost:9116']  # Adjust the target address

Do backups everyhour

Balance partion size
appropriate partition keys


MergeTree Settings: Use settings like merge_with_ttl_timeout, merge_max_block_size, and merge_min_block_size_rows to control the merging of partitions and ensure optimal performance.


System Tables: Regularly check system tables like system.parts and system.parts_columns to monitor the health and number of partitions.


Hot data and cold data strategies

DOCKER


Strategy to STORE THE DATA, ARQUIVE  the data. And In case of need, query thearquived data. 









• Partitioning
CREATE TABLE error_logs (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    severity VARCHAR(10),
    message TEXT,
    stack_trace TEXT,
    context JSONB
) PARTITION BY RANGE (timestamp);

* Recovery:
Restoring data from a backup to bring the database to a specific state.
In case of point-in-time recovery (PITR), it involves applying WAL files to reach the desired state.

• maitanance task. Vacuuming, analyzing, Reindexing, partitioning


You may scrap this metrics with prometeus. 

-- Database size
SELECT pg_size_pretty(pg_database_size('your_database_name'));

-- Table size
SELECT pg_size_pretty(pg_total_relation_size('your_table_name'));


SELECT
    relname as table_name,
    indexrelname as index_name,
    idx_scan as index_scans
FROM
    pg_stat_user_indexes
ORDER BY
    idx_scan DESC;


SELECT
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit)  as heap_hit,
    (sum(heap_blks_hit) - sum(heap_blks_read)) / sum(heap_blks_hit + heap_blks_read) as ratio
FROM
    pg_statio_user_tables;


SELECT
    pid,
    usename,
    locktype,
    relation,
    mode,
    granted
FROM




SELECT
    query,
    calls,
    total_time,
    rows,
    100.0 * total_time / sum(total_time) OVER () as percentage,
    mean_time,
    min_time,
    max_time
FROM
    pg_stat_statements
ORDER BY
    total_time DESC



SELECT
    pid,
    usename,
    application_name,
    client_addr,
    backend_start,
    state,
    query
FROM




SELECT
    relname as table_name,
    seq_scan,
    idx_scan,
    n_tup_ins,
    n_tup_upd,
    n_tup_del
FROM
    pg_stat_user_tables;



SELECT
    datname,
    numbackends,
    xact_commit,
    xact_rollback,
    blks_read,
    blks_hit,
    tup_returned,
    tup_fetched,
    tup_inserted,
    tup_updated,
    tup_deleted
FROM
    pg_stat_database;


    pg_stat_activity;

LIMIT 10;

    pg_locks;




-- Enable the extension
CREATE EXTENSION pg_stat_statements;

-- View query statistics
SELECT
    query,
    calls,
    total_time,
    rows,
    mean_time,
    min_time,
    max_time
FROM
    pg_stat_statements
ORDER BY
    total_time DESC;



-- Enable the extension
CREATE EXTENSION pg_stat_kcache;

-- View kernel-level statistics
SELECT * FROM pg_stat_kcache;




Prometheus can be used to scrape metrics from PostgreSQL using exporters like postgres_exporter.
Grafana can visualize these metrics in customizable dashboards.

Configure the exporter to connect to your PostgreSQL instance.
scrape_configs:
  - job_name: 'postgres'
    static_configs:
      - targets: ['localhost:9187']  # Adjust to your exporter’s address
